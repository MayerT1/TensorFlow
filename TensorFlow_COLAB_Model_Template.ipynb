{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_COLAB_Model_Template",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "olraK1laM33E"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSIfBsgi8dNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Copyright 2019 Google LLC. { display-mode: \"form\" }\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMrH9ZqtmGI9",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow Working Group Main Model Template (updated 10/10/19)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC8adBmw-5m3",
        "colab_type": "text"
      },
      "source": [
        "# Introduction (updated 10/10/19)\n",
        "\n",
        "This is an Earth Engine <> TensorFlow demonstration notebook.  Specifically, this notebook shows:\n",
        "\n",
        "1.   Colab Setup and Authenticate Preparing the data for use in a TensorFlow model.\n",
        "3.   Distingushing Data Lables i.e Categoical/Continous and/or Points/Polygons. \n",
        "4.   Splitting data (Training, Testing, and Validation).\n",
        "5.   Incorperating ModelFeatures via Google Earth Engine Assests\n",
        "6.   Employing a variety of Modeling approaches (Segementation: DNN, CNN, FCNN, UNet; ** Object Detection: UNDER DEVELOPMENT** )\n",
        "7.   Outline Continous/ Categorical outputs process\n",
        "8.   Assessing Model Output: Categorical Overall Accuracy ** TensorBoard: UNDER DEVELOPMENT** \n",
        "9.   Test Models: Validation Data\n",
        "10.  Export, Run Visualize Model(s) through: AI Platform, Google Bucket, and Local approaches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED8HLZZhKr5N",
        "colab_type": "text"
      },
      "source": [
        "# Selecting a Modeling Framework\n",
        "\n",
        "When approaching Neural Networks you should be asking the following questions:\n",
        "\n",
        "1.   What is the preferred input data (Points or Polygons)?\n",
        "2.   Which model to use (i.e CNN, DNN, FCNN, U-Net)?\n",
        "3.   What is the prediction type (Continous or Categorical)?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X43r0CrPKwnG",
        "colab_type": "text"
      },
      "source": [
        "## Workflow: Selecting a Modeling Framework\n",
        "*  Found here for edits: https://docs.google.com/drawings/d/10LpFTTp03OtKxow6S8EnF_dm1QWjEpAMaDQkMD6tcs4/edit?usp=sharing\n",
        "*  to upload -> file-> publish to the web-> Select embeded -> copy in the link\n",
        "\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQmwy5Aqt9QS7JVmNez8JEe9ZFcvCqzTG4xkYpH4Mz3xFtoomPtCTipudddGqUCO15q8D60O9SEBXhV/pub?w=1235&amp;h=923\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3BWYm2uK1AP",
        "colab_type": "text"
      },
      "source": [
        "## TensorFlow Google Ecosystem Workflow (add ecosystem image 10/19/19)\n",
        "\n",
        " For edits: !(https://drive.google.com/open?id=14W51sBKFKP0LPpzrINVsEDQdTwq6dd8i5iDMsxEQGMI\n",
        " \n",
        " <img src=\"https://docs.google.com/drawings/d/e/2PACX-1vSwBaEVIbVCVqUsQxXdykz6HHGWiq3VKeTTwk8tmvHDyShMFFZbDd6wz5yGqwfTuj8fkO71O_WMl5eW/pub?w=1060&amp;h=1015\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl8YT-PJKkeA",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Below are Examples using Points and Polygons input to generate models\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKP9HN-0S0hh",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Example 1: Point, DNN, Categorical (1-Class), Categorical Cross Entropy, Softmax Activation, 1- Node\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI-6FR28I1Es",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Edit: https://docs.google.com/drawings/d/1boEPxc8yLAKQGtReKnKkugh0qf1smXmDTBfeNHKg4xc/edit\n",
        "\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQFVPTzB85yAoPcvYCMnIk-euiQc0SbQe82TLeJx4rbFRnTuQh8N1dCPyqZyyZOlhp2mcvkLuMvJa07/pub?w=921&amp;h=464\">\n",
        "\n",
        "\n",
        "Selecting a Modeling Framework: 1.   What is the preferred input data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_49m_Is-7FwS",
        "colab_type": "text"
      },
      "source": [
        "#### Colab Setup\n",
        "![alt text](https://colab.research.google.com/img/colab_favicon.ico)\n",
        "\n",
        "Colab is a FREE, Jupyter notebook style, python interface. To maximize the efficacy of COLAB:\n",
        ">**Edit-> Notebook settings-> select python 3 and GPU as  Hardware accelerator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiTyR3FNlv-O",
        "colab_type": "text"
      },
      "source": [
        "### Install the Earth Engine client library\n",
        "\n",
        "This only needs to be done once per notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYyTIPLsvMWl",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "!pip install earthengine-api"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qptAXhKmXo_J",
        "colab_type": "text"
      },
      "source": [
        "### Authentication\n",
        "\n",
        "\n",
        "![alt text](https://icons-for-free.com/iconfiles/png/128/key+password+unlock+icon-1320190846512238901.png) **<-Pro Tip icon**\n",
        "\n",
        "\n",
        "  \n",
        "**Exceptionally Important**: \n",
        "To read/write from a Google Cloud Storage bucket/AI Platform, it's necessary to authenticate (**as yourself**).  You'll also need to authenticate as yourself with Earth Engine, so that you'll have access to your scripts, assets, etc.\n",
        "\n",
        "**Protip: Use the same Google account between Colab, Cloud, and GEE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEM3FP4YakJg",
        "colab_type": "text"
      },
      "source": [
        "### Authenticate to Colab and Cloud\n",
        "\n",
        "Identify yourself to Google Cloud, so you have access to storage and other resources.  When you run the code below, it will display a link in the output to an authentication page in your browser.  Follow the link to a page that will let you grant permission to the Cloud SDK to access your resources.  Copy the code from the permissions page back into this notebook and press return to complete the process.\n",
        "\n",
        "(You may need to run this again if you get a credentials error later.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qMKG1hEXuML",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMkNljPGMzyG",
        "colab_type": "text"
      },
      "source": [
        "## Initialize and test the software setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJPDyllIMRAa",
        "colab_type": "text"
      },
      "source": [
        "### Test the Earth Engine installation\n",
        "\n",
        "![alt text](https://www.google.com/images/icons/product/earth_engine-128.png)\n",
        "\n",
        "Authenticate to Earth Engine the same way you did to the Colab notebook.  Specifically, run the code to display a link to a permissions page.  This gives you access to your Earth Engine account.  Copy the code from the Earth Engine permissions page back into the notebook and press return to complete the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuAk34-HMXnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the Earth Engine API and initialize it.\n",
        "import ee\n",
        "\n",
        "ee.Authenticate()\n",
        "ee.Initialize()\n",
        "\n",
        "# Test the earthengine command by getting help on upload.\n",
        "!earthengine upload image -h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ70EsoWND_0",
        "colab_type": "text"
      },
      "source": [
        "### Test the TensorFlow installation\n",
        "\n",
        "![alt text](\n",
        "https://data.apkhere.com/5d/cc.nextlabs.tensorflow/1.0.3/icon.png!s)\n",
        "\n",
        "The default public runtime already has the tensorflow libraries we need installed.  Before any operations from the TensorFlow API are used, import TensorFlow and enable eager execution.  This provides an imperative interface that can help with debugging.  See the [TensorFlow eager execution guide](https://www.tensorflow.org/guide/eager) or the [`tf.enable_eager_execution()` docs](https://www.tensorflow.org/api_docs/python/tf/enable_eager_execution) for details. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1PrYRLaVw_g",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8Xcvjp6cLOL",
        "colab_type": "text"
      },
      "source": [
        "### Test the Folium installation\n",
        "![alt text](https://python-visualization.github.io/folium/_images/folium_logo.jpg)\n",
        "\n",
        "The default public runtime already has the Folium library we will use for visualization.  Import the library, check the version, and define the URL where Folium will look for Earth Engine generated map tiles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiVgOXzBZJSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import folium\n",
        "print(folium.__version__)\n",
        "\n",
        "# Define the URL format used for Earth Engine generated map tiles.\n",
        "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHFdCmXCd4gm",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess Imagery and Input Data then export as GEE Asset\n",
        "![alt text](https://icons-for-free.com/iconfiles/png/128/key+password+unlock+icon-1320190846512238901.png)![alt text](http://www.i2clipart.com/cliparts/b/3/f/6/clipart-stop-sign-128x128-b3f6.png)![alt text](data:image/png;base64,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)\n",
        "\n",
        "![alt text](https://www.google.com/images/icons/product/earth_engine-128.png)\n",
        "![alt text](https://python-visualization.github.io/folium/_images/folium_logo.jpg)\n",
        "\n",
        "Examples below displays the Sentinel 2 Surface Reflectance composite imagery for a small area in Vietnam which will be used to model rice.\n",
        "\n",
        "Second code block displays the FeatureCollection containing input rice points with a label \n",
        "\n",
        "Generating complte composites and prepped input to call into the workflow is a **KEY** method for capitalizing on the Google modeling ecosystem.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7syUtsX6nEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use folium to visualize the imagery.\n",
        "# the image is in float\n",
        "image = ee.Image('users/phamngochairs/NB_S2A_Spectral_2018_2018')\n",
        "vietnam = ee.FeatureCollection('users/servirmekong/countries/VNM_adm1')\n",
        "deltaProvience = ['Dong Thap', 'Tien Giang', 'Vinh Long', 'Tra Vinh', 'An Giang', 'Can Tho', 'Hau Giang', 'Soc Trang', 'Bac Lieu', 'Kien Giang', 'Ben Tre', 'Long An'];\n",
        "geometry = vietnam.filter(ee.Filter.inList('VARNAME_1', deltaProvience))\n",
        "bounds = geometry.geometry().bounds()\n",
        "\n",
        "mapid = image.clip(geometry).getMapId({'bands': ['red', 'green', 'blue'], 'min': 0.05, 'max': 0.3})\n",
        "map = folium.Map(location=[10.0476915, 105.1530271], zoom_start=9)\n",
        "folium.TileLayer(\n",
        "    tiles=EE_TILES.format(**mapid),\n",
        "    attr='Google Earth Engine',\n",
        "    overlay=True,\n",
        "    name='mediod composite',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx0upDmQU05S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use folium to visualize the imagery.\n",
        "# the image is in float\n",
        "image = ee.Image('users/phamngochairs/NB_S2A_Spectral_2018_2018')\n",
        "vietnam = ee.FeatureCollection('users/servirmekong/countries/VNM_adm1')\n",
        "deltaProvience = ['Dong Thap', 'Tien Giang', 'Vinh Long', 'Tra Vinh', 'An Giang', 'Can Tho', 'Hau Giang', 'Soc Trang', 'Bac Lieu', 'Kien Giang', 'Ben Tre', 'Long An'];\n",
        "geometry = vietnam.filter(ee.Filter.inList('VARNAME_1', deltaProvience))\n",
        "bounds = geometry.geometry().bounds()\n",
        "\n",
        "mapid = image.clip(geometry).getMapId({'bands': ['red', 'green', 'blue'], 'min': 0.05, 'max': 0.3})\n",
        "map = folium.Map(location=[10.0476915, 105.1530271], zoom_start=9)\n",
        "folium.TileLayer(\n",
        "    tiles=EE_TILES.format(**mapid),\n",
        "    attr='Google Earth Engine',\n",
        "    overlay=True,\n",
        "    name='mediod composite',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPEXGC10KGKE",
        "colab_type": "text"
      },
      "source": [
        "### Point Data GEE\n",
        "![alt text](https://www.google.com/images/icons/product/earth_engine-128.png)\n",
        "\n",
        "*   Building off the Vietnam rice example:\n",
        "**  Commented out workflow displays how point data can be **Labeled, Prepped, and Split**\n",
        "*   This is for reference to show how the input data needs to be **Labeled, Prepped, and Split**\n",
        "*   Moving forward for **YOUR** modeling work, add in a new code block and **Labeled, Prepped, and Split** as needed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ3a_0PtUmcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label = 'rice'\n",
        "\n",
        "vietnam = ee.FeatureCollection('users/servirmekong/countries/VNM_adm1')\n",
        "compositeSentinel2 = ee.Image('users/phamngochairs/NB_S2A_Spectral_2018_2018')\n",
        "\n",
        "# export training data\n",
        "aquaculture =  ee.FeatureCollection(\"users/servirmekong/referenceData/aquaculture\");\n",
        "barren = ee.FeatureCollection(\"users/servirmekong/referenceData/barren\");\n",
        "cropland = ee.FeatureCollection(\"users/servirmekong/referenceData/cropland\");\n",
        "deciduousForest = ee.FeatureCollection(\"users/servirmekong/referenceData/deciduousForest\");\n",
        "evergreen = ee.FeatureCollection(\"users/servirmekong/referenceData/evergreen_forest\");\n",
        "floodedForest = ee.FeatureCollection(\"users/servirmekong/referenceData/floodedForest\");\n",
        "plantations = ee.FeatureCollection(\"users/servirmekong/referenceData/plantations\");\n",
        "grass = ee.FeatureCollection(\"users/servirmekong/referenceData/grass\");\n",
        "mangroves = ee.FeatureCollection(\"users/servirmekong/referenceData/mangrove\");\n",
        "mixedForest = ee.FeatureCollection(\"users/servirmekong/referenceData/mixedForest\");\n",
        "plantations = ee.FeatureCollection(\"users/servirmekong/referenceData/plantations\");\n",
        "rice = ee.FeatureCollection(\"users/servirmekong/referenceData/rice\")\n",
        "shrub = ee.FeatureCollection(\"users/servirmekong/referenceData/shrub\")\n",
        "snow = ee.FeatureCollection(\"users/servirmekong/referenceData/snow\")\n",
        "water = ee.FeatureCollection(\"users/servirmekong/referenceData/water\")\n",
        "imperv = ee.FeatureCollection(\"users/servirmekong/referenceData/imperv\")\n",
        "wetlands = ee.FeatureCollection(\"users/servirmekong/referenceData/wetlands\")\n",
        "noRice = aquaculture.merge(barren).merge(cropland).merge(deciduousForest).merge(evergreen).merge(floodedForest).merge(plantations).merge(grass).merge(mangroves).merge(mixedForest).merge(shrub).merge(snow).merge(water).merge(imperv).merge(wetlands)\n",
        "\n",
        "def _set_true_feature(feature):\n",
        "    return feature.set(label, 1)\n",
        "  \n",
        "def _set_false_feature(feature):\n",
        "    return feature.set(label, 0)\n",
        "\n",
        "rice = rice.filterBounds(bounds).map(_set_true_feature)\n",
        "\n",
        "noRice = noRice.filterBounds(bounds).map(_set_false_feature)\n",
        "\n",
        "features = rice.merge(noRice)\n",
        "\n",
        "# Sample the image at the points and add a random column.\n",
        "sample = compositeSentinel2.sampleRegions(\n",
        "  collection = features, properties = [label], scale = 10).randomColumn()\n",
        "\n",
        "# Partition the sample approximately 70-30.\n",
        "training = sample.filter(ee.Filter.lt('random', 0.7))\n",
        "testing = sample.filter(ee.Filter.gte('random', 0.7))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUhBvoQxKXA8",
        "colab_type": "text"
      },
      "source": [
        "### Point Data Cloud (continued)\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\"><img src=\"https://docs.google.com/drawings/d/e/2PACX-1vSd57P-mUkAvVGyYjrsq-rQsSSIS4aTBN0mMvuX2o9PGxbWXjeEEU9GDSATr8aXZ0Fwp79a3Muypo4u/pub?w=100&amp;h=100\">\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://icons-for-free.com/iconfiles/png/128/key+password+unlock+icon-1320190846512238901.png)\n",
        "![alt text](http://www.i2clipart.com/cliparts/b/3/f/6/clipart-stop-sign-128x128-b3f6.png)\n",
        "\n",
        "\n",
        "Now that there's training and testing data in Earth Engine and you've inspected a couple examples to ensure that the information you need is present, it's time to materialize the datasets in a place where the TensorFlow model has access to them.  You can do that by exporting the training and testing datasets to tables in TFRecord format ([learn more about TFRecord format](https://www.tensorflow.org/tutorials/load_data/tf-records)) in a Cloud Storage bucket ([learn more about creating Cloud Storage buckets](https://cloud.google.com/storage/docs/creating-buckets)).  Note that you need to have write access to the Cloud Storage bucket where the files will be output.\n",
        "\n",
        "**Go to -> console.cloud.google.com-> select \"storage\"-> create a bucket**\n",
        "\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vRXUEGZzCfJP86_LxzWeCCCGyMwYlED4oN0VBlPraYkZPciDXUTcLrvYbh9YQ9p-QGR1cwtv6PVZBqm/pub?w=764&amp;h=461\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncCimj9mKaBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Just use this bucket, to which you have read access.\n",
        "outputBucket = 'ricebucket'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgm-hE8qA6UI",
        "colab_type": "text"
      },
      "source": [
        "**Print the feature names** below to display what you have -> ee.batchExport.table.toCloudStorage()\n",
        "\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">![alt text](https://colab.research.google.com/img/colab_favicon.ico)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcBLqCpeMIze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bands = compositeSentinel2.bandNames().getInfo()\n",
        "\n",
        "# Names for output files.\n",
        "trainFilePrefix = 'tf_rice_training'\n",
        "testFilePrefix = 'tf_rice_testing'\n",
        "\n",
        "# This is list of all the properties we want to export.\n",
        "featureNames = list(bands)\n",
        "featureNames.append(label)\n",
        "print(featureNames)\n",
        "\n",
        "# Create the tasks.\n",
        "trainingTask = ee.batch.Export.table.toCloudStorage(\n",
        "  collection=training,\n",
        "  description='Training Export',\n",
        "  fileNamePrefix=trainFilePrefix,\n",
        "  bucket=outputBucket,\n",
        "  fileFormat='TFRecord',\n",
        "  selectors=featureNames)\n",
        "\n",
        "testingTask = ee.batch.Export.table.toCloudStorage(\n",
        "  collection=testing,\n",
        "  description='Testing Export',\n",
        "  fileNamePrefix=testFilePrefix,\n",
        "  bucket=outputBucket,\n",
        "  fileFormat='TFRecord',\n",
        "  selectors=featureNames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGEaooidGqmF",
        "colab_type": "text"
      },
      "source": [
        "**Start the task**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx6h0AX5GpRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start the tasks.\n",
        "trainingTask.start()\n",
        "testingTask.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxWwKlRhLsCS",
        "colab_type": "text"
      },
      "source": [
        "### Monitor task progress\n",
        "\n",
        "![alt text](https://www.google.com/images/icons/product/earth_engine-128.png)![alt text](https://image.flaticon.com/icons/png/128/98/98673.png)<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">\n",
        "\n",
        "You can see all your Earth Engine tasks by listing them.  It's also useful to repeatedly poll a task so you know when it's done.  Here we can do that because this is a relatively quick export.  Be careful when doing this with large exports because it will block the notebook from running other cells until this one completes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjW2irjzL49k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print all tasks.\n",
        "print(ee.batch.Task.list())\n",
        "\n",
        "# Poll the training task until it's done.\n",
        "import time \n",
        "while trainingTask.active():\n",
        "  print('Polling for task (id: {}).'.format(trainingTask.id))\n",
        "  time.sleep(30)\n",
        "print('Done with training export.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b0rVn3mMBSy",
        "colab_type": "text"
      },
      "source": [
        "### Check existence of the exported files\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">\n",
        "\n",
        "If you've seen the status of the export tasks change to `COMPLETED`, then check for the existince of the files in the output Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFp-ROyQMCYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fileNameSuffix = '.tfrecord.gz'\n",
        "trainFilePath = 'gs://' + outputBucket + '/' + trainFilePrefix + fileNameSuffix\n",
        "testFilePath = 'gs://' + outputBucket + '/' + testFilePrefix + fileNameSuffix\n",
        "\n",
        "print('Found training file.' if tf.gfile.Exists(trainFilePath) \n",
        "    else 'No training file found.')\n",
        "print('Found testing file.' if tf.gfile.Exists(testFilePath) \n",
        "    else 'No testing file found.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQVBZrHTOzJX",
        "colab_type": "text"
      },
      "source": [
        "### Export the imagery\n",
        "\n",
        "![alt text](https://www.google.com/images/icons/product/earth_engine-128.png)![alt text](https://image.flaticon.com/icons/png/128/98/98673.png)<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">\n",
        "\n",
        "You can also export imagery using TFRecord format.  Specifically, export whatever imagery you want to be classified by the trained model into the output Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKX7pWuaO0va",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageFilePrefix = 'tf_rice_image'\n",
        "\n",
        "# Specify patch and file dimensions.\n",
        "imageExportFormatOptions = {\n",
        "  'patchDimensions': [256, 256],\n",
        "  'maxFileSize': 104857600,\n",
        "  'compressed': True\n",
        "}\n",
        "\n",
        "# Setup the task.\n",
        "imageTask = ee.batch.Export.image.toCloudStorage(\n",
        "  image=compositeSentinel2,\n",
        "  description='Image Export',\n",
        "  fileNamePrefix=imageFilePrefix,\n",
        "  bucket=outputBucket,\n",
        "  scale=10,\n",
        "  fileFormat='TFRecord',\n",
        "  region=bounds.getInfo()['coordinates'],\n",
        "  formatOptions=imageExportFormatOptions,\n",
        "  maxPixels=1E13,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxLfSf3dQVth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start the task.\n",
        "imageTask.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2puANsJQaEp",
        "colab_type": "text"
      },
      "source": [
        "### Monitor task progress\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">\n",
        "\n",
        "Before making predictions, we need the image export to finish, so block until it does.  This might take a few minutes..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfVMvcL4Qa2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "while imageTask.active():\n",
        "  print('Polling for task (id: {}).'.format(imageTask.id))\n",
        "  time.sleep(30)\n",
        "print('Done with image export.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KopNNUGmRVE3",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation and pre-processing \n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vSjN6dsseY5dVuMePJvMl4Od2IKLCN5rL4yQuUpFOfeKXzFrJsa6oTUr_0Zdo-9oKamOH_f5rIjuN0r/pub?w=490&amp;h=189\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIaiO80NRcQA",
        "colab_type": "text"
      },
      "source": [
        "### Read into a `tf.data.Dataset`\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">\n",
        "\n",
        "Here we are going to read a file in Cloud Storage into a `tf.data.Dataset`.  ([these TensorFlow docs](https://www.tensorflow.org/guide/premade_estimators#create_input_functions) explain more about reading data into a `Dataset`).  Check that you can read examples from the file.  The purpose here is to ensure that we can read from the file without an error.  The actual content is not necessarily human readable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACXhrDuPRdrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dataset from the TFRecord file in Cloud Storage.\n",
        "trainDataset = tf.data.TFRecordDataset(trainFilePath, compression_type='GZIP')\n",
        "# Print the first record to check.\n",
        "print(iter(trainDataset).next())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go61XzQkTNZM",
        "colab_type": "text"
      },
      "source": [
        "### Define the structure of your data\n",
        "![alt text](https://icons-for-free.com/iconfiles/png/128/key+password+unlock+icon-1320190846512238901.png)![alt text](https://colab.research.google.com/img/colab_favicon.ico)\n",
        "### [slide](https://docs.google.com/presentation/d/1gVMt2QPX-KWmQ0B9TYkAxksU6fWWac7W7roWChseJe0/edit#slide=id.g626cc8ea58_0_169)\n",
        "\n",
        "For parsing the exported TFRecord files, `featuresDict` is a mapping between feature names (recall that `featureNames` contains the band and label names) and **`float32`** [`tf.io.FixedLenFeature`](https://www.tensorflow.org/api_docs/python/tf/io/FixedLenFeature) objects.  This mapping is necessary for telling TensorFlow how to read data in a TFRecord file into tensors.  **Specifically, all numeric data exported from Earth Engine is exported as `float32`**.\n",
        "\n",
        "(Note: *features* in the TensorFlow context (i.e. [`feature.proto`](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/core/example/feature.proto)) are not to be confused with Earth Engine features (i.e. [`ee.Feature`](https://developers.google.com/earth-engine/api_docs#eefeature)), where the former is a protocol message type for serialized data input to the model and the latter is a geometry-based geographic data structure.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YFs4mkLTfgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint\n",
        "# List of fixed-length features, all of which are float32.\n",
        "columns = [\n",
        "  tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for k in featureNames\n",
        "]\n",
        "\n",
        "# Dictionary with names as keys, features as values.\n",
        "featuresDict = dict(zip(featureNames, columns))\n",
        "\n",
        "pprint(featuresDict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1Rmki9pGk-J",
        "colab_type": "text"
      },
      "source": [
        "###Parse the dataset\n",
        "Edit: https://docs.google.com/drawings/d/1MK9EYDGM1hWcX1-q-nUfcFeorY-diqVXVsWa21WcSMc/edit\n",
        "\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQssp5M98v7bllX6hN1N1Std6y78aD9R78oBWifGV1W8DfN02EHeMluW57F_SX1i9-TDcXIow9K85jM/pub?w=169&amp;h=168\"> ![alt text](https://colab.research.google.com/img/colab_favicon.ico)\n",
        "### [slide](https://docs.google.com/presentation/d/1gVMt2QPX-KWmQ0B9TYkAxksU6fWWac7W7roWChseJe0/edit#slide=id.g626cc8ea58_0_169)\n",
        "\n",
        "Now we need to make a parsing function for the data in the TFRecord files.  The data comes in flattened 2D arrays per record and we want to use the first part of the array for input to the model and the last element of the array as the class label.  The parsing function reads data from a serialized `Example` proto (i.e. [`example.proto`](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/core/example/example.proto)) into a dictionary in which the keys are the feature names and the values are the tensors storing the value of the features for that example.  ([Learn more about parsing `Example` protocol buffer messages](https://www.tensorflow.org/programmers_guide/datasets#parsing_tfexample_protocol_buffer_messages))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgfg1TFuG5Pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"The parsing function.\n",
        "\n",
        "  Read a serialized example into the structure defined by featuresDict.\n",
        "\n",
        "  Args:\n",
        "    example_proto: a serialized Example.\n",
        "  \n",
        "  Returns: \n",
        "    A tuple of the predictors dictionary and the label, cast to an `int32`.\n",
        "  \"\"\"\n",
        "  parsed_features = tf.io.parse_single_example(example_proto, featuresDict)\n",
        "  labels = parsed_features.pop(label)\n",
        "  return parsed_features, tf.cast(labels, tf.int32)\n",
        "\n",
        "# Map the function over the dataset.\n",
        "parsedDataset = trainDataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "\n",
        "# Print the first parsed record to check.\n",
        "pprint(iter(parsedDataset).next())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBz8BVgsPR9Z",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://icons-for-free.com/iconfiles/png/128/key+password+unlock+icon-1320190846512238901.png)\n",
        "\n",
        "Note that each record of the parsed dataset contains a tuple.  The first element of the tuple is a dictionary with bands for keys and the numeric value of the bands for values.  The second element of the tuple is a class label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIcbKIAIPZ01",
        "colab_type": "text"
      },
      "source": [
        "### Optional, Example how to add more \"features\" mid-workflow?\n",
        "**Create additional features**\n",
        "\n",
        "![alt text](https://www.google.com/images/icons/product/earth_engine-128.png)![alt text](https://colab.research.google.com/img/colab_favicon.ico)\n",
        "\n",
        "Another thing we might want to do as part of the input process is to create new features, for example NDVI, a vegetation index computed from reflectance in two spectral bands.  Here are some helper functions for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6JJfcBQPlNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalizedDifference(a, b):\n",
        "  nd = (a - b) / (a + b)\n",
        "  nd_inf = (a - b) / (a + b + 0.00000001)\n",
        "  return tf.where(tf.is_finite(nd), nd, nd_inf)\n",
        "\n",
        "def ratio(a, b):\n",
        "  nd = a / b\n",
        "  nd_inf = a / (b + 0.00000001)\n",
        "  return tf.where(tf.is_finite(nd), nd, nd_inf)\n",
        "\n",
        "def EVI(NIR, RED, BLUE):\n",
        "\t#Add Enhanced Vegetation Index (EVI)\n",
        "\tevi = 2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))\n",
        "\treturn evi\n",
        "\n",
        "\n",
        "def SAVI(NIR,RED):\n",
        "\t#Add Enhanced Vegetation Index (EVI)\n",
        "\tsavi =(NIR - RED) * (1 + 0.5)/(NIR + RED + 0.5)\n",
        "\treturn savi\n",
        "\n",
        "\n",
        "def IBI(RED,GREEN,SWIR1,NIR):\n",
        "\t#Add Index-Based Built-Up Index (IBI)\n",
        "\tibiA = (2 * SWIR1) / (SWIR1 + NIR)\n",
        "\tibiB = (NIR / (NIR + RED)) + (GREEN / (GREEN + SWIR1))\n",
        "\t\n",
        "\tibi = normalizedDifference(ibiA,ibiB)\n",
        "\treturn ibi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bKB5kT7P172",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def addFeatures(features, label):\n",
        "\n",
        "\tfeatures['nd_blue_green'] = normalizedDifference(features['blue'], features['green'])\n",
        "\tfeatures['nd_blue_nir'] = normalizedDifference(features['blue'], features['nir'])\n",
        "\tfeatures['ndvi'] = normalizedDifference(features['nir'], features['red'])\n",
        "\tfeatures['ndwi'] = normalizedDifference(features['green'], features['nir'])\n",
        "\tfeatures['nd_swir1_swir2'] = normalizedDifference(features['swir1'], features['swir2'])\n",
        "\tfeatures['nd_nir_swir2'] = normalizedDifference(features['nir'], features['swir2'])\n",
        "\tfeatures['ratio_swir1_nir'] = ratio(features['swir1'], features['nir'])\n",
        "\tfeatures['ratio_red_swir1'] = ratio(features['red'], features['swir1'])\n",
        "\tfeatures['evi'] = EVI(features['nir'], features['red'],features['blue'])\n",
        "\tfeatures['savi'] = SAVI(features['nir'], features['red'])\n",
        "\tfeatures['ibi'] = IBI(features['red'],features['green'],features['swir1'],features['nir'])\n",
        "\n",
        "\treturn features, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOTscQx7R8vd",
        "colab_type": "text"
      },
      "source": [
        "## DNN Model setup\n",
        "\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vTM_GRj4rQBmXKKDfi_haM8PP_RfAkmCX_i8v-lsQ6y_mHmkbbsn1Oj77GWqhqazKVvLHQSGTccezdi/pub?w=296&amp;h=196\">\n",
        "\n",
        "The basic workflow for classification in TensorFlow is:\n",
        "\n",
        "1.  Create the model.\n",
        "2.  Train the model (i.e. `fit()`).\n",
        "3.  Use the trained model for inference (i.e. `predict()`).\n",
        "\n",
        "Here we'll create a `Sequential` neural network model using Keras.  This simple model is inspired by examples in:\n",
        "\n",
        "* [The TensorFlow Get Started tutorial](https://www.tensorflow.org/tutorials/)\n",
        "* [The TensorFlow Keras guide](https://www.tensorflow.org/guide/keras#build_a_simple_model)\n",
        "* [The Keras `Sequential` model examples](https://keras.io/getting-started/sequential-model-guide/#multilayer-perceptron-mlp-for-multi-class-softmax-classification)\n",
        "\n",
        "Note that the model used here is purely for demonstration purposes and hasn't gone through any performance tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhJZJQzCSFbv",
        "colab_type": "text"
      },
      "source": [
        "### Create the Keras model\n",
        "\n",
        "![alt text](https://colab.research.google.com/img/colab_favicon.ico)\n",
        "\n",
        "\n",
        "edit: https://docs.google.com/drawings/d/1EKLXzi7GrCtayakiiki1GcKDXaumcq_ksGiEjBnywQE/edit\n",
        "\n",
        "\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vTjGDbb6wQzLUKtFDgjkc9nXnF8X1dMXWK5OuYKVK8suuu1r1Hy3DCFWlyrgwhuU0MI7M5jfMcGXoco/pub?w=345&amp;h=332\">\n",
        "\n",
        "Before we create the model, there's still a wee bit of pre-processing to get the data into the right input shape and a format that can be used with cross-entropy loss.  Specifically, Keras expects a list of inputs and a one-hot vector for the class. (See [the Keras loss function docs](https://keras.io/losses/), [the TensorFlow categorical identity docs](https://www.tensorflow.org/guide/feature_columns#categorical_identity_column) and [the `tf.one_hot` docs](https://www.tensorflow.org/api_docs/python/tf/one_hot) for details).  \n",
        "\n",
        "Here we will use a simple neural network model with a 64 node hidden layer, a dropout layer and an output layer.  Once the dataset has been prepared, define the model, compile it, fit it to the training data.  See [the Keras `Sequential` model guide](https://keras.io/getting-started/sequential-model-guide/) for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4ur0WV-SJH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# How many classes there are in the model.\n",
        "nClasses = 2\n",
        "\n",
        "# Add features .\n",
        "inputDataset = parsedDataset.map(addFeatures)\n",
        "\n",
        "# Keras requires inputs as a tuple.  Note that the inputs must be in the\n",
        "# right shape.  Also note that to use the categorical_crossentropy loss,\n",
        "# the label needs to be turned into a one-hot vector.\n",
        "def toTuple(dict, label):\n",
        "  #return tf.transpose(list(dict.values())), tf.one_hot(indices=label, depth=nClasses)\n",
        "  return (tf.expand_dims(tf.transpose(list(dict.values())), 1),\n",
        "          tf.expand_dims(tf.one_hot(indices=label, depth=nClasses), 1))\n",
        "\n",
        "# Repeat the input dataset as many times as necessary in batches.\n",
        "inputDataset = inputDataset.map(toTuple).shuffle(1000).batch(100)\n",
        "\n",
        "# Define the layers in the model.\n",
        "model = tf.keras.models.Sequential([\n",
        "  #tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
        "  #tf.keras.layers.Dropout(0.1),\n",
        "  #tf.keras.layers.Dense(nClasses, activation=tf.nn.softmax)\n",
        "  tf.keras.layers.Input((None, None, 17,)),\n",
        "  tf.keras.layers.Conv2D(64, (1, 1), activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv2D(32, (1, 1), activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Conv2D(nClasses, (1, 1), activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "# Compile the model with the specified loss function.\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.0075),#'adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the model to the training data.\n",
        "# Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.\n",
        "training = model.fit(x=inputDataset, epochs=50)\n",
        "\n",
        "%pylab inline\n",
        "plot(training.history['loss'],'x--')\n",
        "plot(training.history['acc'], 'o--')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_YLlOywXm89",
        "colab_type": "text"
      },
      "source": [
        "###Check model accuracy on the test set\n",
        "\n",
        "![alt text](https://colab.research.google.com/img/colab_favicon.ico)\n",
        "\n",
        "Now that we have a trained model, we can evaluate it using the test dataset.  To do that, read and prepare the test dataset in the same way as the training dataset.  Here we specify a batch sie of 1 so that each example in the test set is used exactly once to compute model accuracy.  For model steps, just specify a number larger than the test dataset size (ignore the warning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHhKiDdjXozq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testDataset = (\n",
        "  tf.data.TFRecordDataset(testFilePath, compression_type='GZIP')\n",
        "    .map(parse_tfrecord, num_parallel_calls=5)\n",
        "    .map(addFeatures)\n",
        "    .map(toTuple)\n",
        "    .batch(1)\n",
        ")\n",
        "\n",
        "model.evaluate(testDataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SjlmJP0Xy8w",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://icons-for-free.com/iconfiles/png/128/key+password+unlock+icon-1320190846512238901.png) <img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">\n",
        "\n",
        "Note that the notebook VM is sometimes not heavy-duty enough to get through a whole training job, especially if you have a large buffer size or a large number of epochs. You can still use this notebook for training, but may need to set up an alternative VM (learn more) for production use. Alternatively, you can package your code for running large training jobs on Google's AI Platform as described here. The following code loads a pre-trained model, which you can use for predictions right away.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_q5RU1cX0yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_DIR = 'gs://ricebucket/model'\n",
        "tf.contrib.saved_model.save_keras_model(model, MODEL_DIR)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr681E4Nb_QO",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the model for making predictions in Earth Engine\n",
        "\n",
        "![alt text](https://icons-for-free.com/iconfiles/png/128/key+password+unlock+icon-1320190846512238901.png)\n",
        "\n",
        "Before we can use the model in Earth Engine, it needs to be hosted by AI Platform.  But before we can host the model on AI Platform we need to ***EEify*** (a new word!) it.  The EEification process merely appends some extra operations to the input and outputs of the model in order to accomdate the interchange format between pixels from Earth Engine (float32) and inputs to AI Platform (base64).  (See [this doc](https://cloud.google.com/ml-engine/docs/online-predict#binary_data_in_prediction_input) for details.)  \n",
        "\n",
        "### Earth Engine model prepare\n",
        "\n",
        "![alt text](https://www.google.com/images/icons/product/earth_engine-128.png)![alt text](https://image.flaticon.com/icons/png/128/98/98673.png)<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">\n",
        "\n",
        "The EEification process is handled for you using the Earth Engine command `earthengine model prepare`.  To use that command, we need to specify the input and output model directories and the name of the input and output nodes in the TensorFlow computation graph.  We can do all that programmatically:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s06PjUtpccR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.tools import saved_model_utils\n",
        "\n",
        "meta_graph_def = saved_model_utils.get_meta_graph_def(MODEL_DIR, 'serve')\n",
        "inputs = meta_graph_def.signature_def['serving_default'].inputs\n",
        "outputs = meta_graph_def.signature_def['serving_default'].outputs\n",
        "\n",
        "# Just get the first thing(s) from the serving signature def.  i.e. this\n",
        "# model only has a single input and a single output.\n",
        "input_name = None\n",
        "for k,v in inputs.items():\n",
        "  input_name = v.name\n",
        "  break\n",
        "\n",
        "output_name = None\n",
        "for k,v in outputs.items():\n",
        "  output_name = v.name\n",
        "  break\n",
        "\n",
        "# Make a dictionary that maps Earth Engine outputs and inputs to \n",
        "# AI Platform inputs and outputs, respectively.\n",
        "import json\n",
        "input_dict = \"'\" + json.dumps({input_name: \"array\"}) + \"'\"\n",
        "output_dict = \"'\" + json.dumps({output_name: \"rice\"}) + \"'\"\n",
        "\n",
        "# Put the EEified model next to the trained model directory.\n",
        "EEIFIED_DIR = 'gs://ricebucket/eeified'\n",
        "#PROJECT = 'ee-tf-sandbox'\n",
        "PROJECT = 'tfproject'\n",
        "\n",
        "# You need to set the project before using the model prepare command.\n",
        "!earthengine set_project {PROJECT}\n",
        "!earthengine model prepare --source_dir {MODEL_DIR} --dest_dir {EEIFIED_DIR} --input {input_dict} --output {output_dict}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXc37MdFdD8e",
        "colab_type": "text"
      },
      "source": [
        "## Perform inference using the trained model in Earth Engine\n",
        "\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">\n",
        "\n",
        "Before it's possible to get predictions from the trained and EEified model, it needs to be deployed on AI Platform.\n",
        "\n",
        "1. The first step is to create the model.\n",
        "\n",
        "2. The second step is to create a version.  See [this guide](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models) for details.  Note that models and versions can be monitored from the [AI Platform models page](http://console.cloud.google.com/ai-platform/models) of the Cloud Console.  \n",
        "\n",
        "\n",
        "\n",
        "1. Open the AI Platform models page in the GCP Console:\n",
        "\n",
        "OPEN MODELS IN THE GCP CONSOLE\n",
        "\n",
        "2. If needed, create the model to add your new version to:\n",
        "\n",
        "Click the New Model button at the top of the Models page. This brings you to the Create model page.\n",
        "\n",
        "Enter a unique name for your model in the Model name box. Optionally, enter a description for your model in the Description field.\n",
        "\n",
        "Click Create.\n",
        "\n",
        "Verify that you have returned to the Models page, and that your new model appears in the list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74ZtouRmdFzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_NAME = 'rice_dnn_model'\n",
        "VERSION_NAME = 'v' + str(int(time.time()))\n",
        "print('Creating version: ' + VERSION_NAME)\n",
        "\n",
        "#!gcloud ai-platform models create {MODEL_NAME} --project {PROJECT}\n",
        "!gcloud ai-platform versions create {VERSION_NAME} \\\n",
        "  --project {PROJECT} \\\n",
        "  --model {MODEL_NAME} \\\n",
        "  --origin {EEIFIED_DIR} \\\n",
        "  --runtime-version=1.14 \\\n",
        "  --framework \"TENSORFLOW\" \\\n",
        "  --python-version=3.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEnu-aU2l3y2",
        "colab_type": "text"
      },
      "source": [
        "**testingspace--tim-10-17-19**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVFZArrwl9DH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_NAME = 'rice_dnn_model'\n",
        "VERSION_NAME = 'v10172019'\n",
        "print('Creating version: ' + VERSION_NAME)\n",
        "\n",
        "#!gcloud ai-platform models create {MODEL_NAME} --project {PROJECT}\n",
        "!gcloud ai-platform versions create {VERSION_NAME} \\\n",
        "  --project {PROJECT} \\\n",
        "  --model {MODEL_NAME} \\\n",
        "  --origin {EEIFIED_DIR} \\\n",
        "  --runtime-version=1.14 \\\n",
        "  --framework \"TENSORFLOW\" \\\n",
        "  --python-version=3.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQw7A1MNd8LT",
        "colab_type": "text"
      },
      "source": [
        "There is now a trained model, prepared for serving to Earth Engine, hosted and versioned on AI Platform.  We can now connect Earth Engine directly to the trained model for inference.  You do that with the **`ee.Model.fromAiPlatformPredictor`** command.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### `ee.Model.fromAiPlatformPredictor`\n",
        "For this command to work, we need to know a lot about the model.  To connect to the model, you need to know the name and version.\n",
        "\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">![alt text](https://image.flaticon.com/icons/png/128/98/98673.png)![alt text](https://www.google.com/images/icons/product/earth_engine-128.png) ![alt text](https://python-visualization.github.io/folium/_images/folium_logo.jpg)\n",
        "\n",
        "\n",
        "### Inputs\n",
        "You need to be able to recreate the imagery on which it was trained in order to perform inference.  Specifically, you need to create an array-valued input from the scaled data and use that for input.  (Recall that the new input node is named `array`, which is convenient because the array image has one band, named `array` by default.)  The inputs will be provided as 144x144 patches (`inputTileSize`), at 30-meter resolution (`proj`), but 8 pixels will be thrown out (`inputOverlapSize`) to minimize boundary effects.\n",
        "\n",
        "### Outputs\n",
        "The output (which you also need to know), is a single float band named `rice`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjMmlViZxvoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the trained model and use it for prediction.\n",
        "model = ee.Model.fromAiPlatformPredictor(\n",
        "    projectName = PROJECT,\n",
        "    modelName = MODEL_NAME,\n",
        "    version = VERSION_NAME,\n",
        "    inputTileSize = [144, 144],\n",
        "    inputOverlapSize = [8, 8],\n",
        "    proj = ee.Projection('EPSG:4326').atScale(30),\n",
        "    fixInputProj = True,\n",
        "    outputBands = {'rice': {\n",
        "        'type': ee.PixelType.float()\n",
        "      }\n",
        "    }\n",
        ")\n",
        "predictions = model.predictImage(image.toArray())\n",
        "\n",
        "# Use folium to visualize the input imagery and the predictions.\n",
        "mapid = compositeSentinel2.getMapId({'bands': ['red', 'green', 'blue'], 'min': 0.05, 'max': 0.3})\n",
        "map = folium.Map(location=[10.0476915, 105.1530271], zoom_start=9)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=EE_TILES.format(**mapid),\n",
        "    attr='Google Earth Engine',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "mapid = predictions.getMapId({'min': 0, 'max': 1})\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=EE_TILES.format(**mapid),\n",
        "    attr='Google Earth Engine',\n",
        "    overlay=True,\n",
        "    name='predictions',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olraK1laM33E",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Example 2: Polyon, U-Net, etc...........Categorical (1-Class), Categorical Cross Entropy, Softmax Activation, 1- Node\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cg-VJIjzVSI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "**edit:** https://docs.google.com/drawings/d/1PCr_CTWBHlWWNwE6qiL2dSvE85W7gMiW90tVmQvpKOI/edit\n",
        "\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQluuFI6IMIwegv3PqvNqtLm_Lp9nlzBbcnpHvJL_-vi-9COmPqCGPVEKIH6VVtNlABC0FEfbuKmqbL/pub?w=921&amp;h=464\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMrF7BG-3L3A",
        "colab_type": "text"
      },
      "source": [
        "## Polygon Example Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAregv0W_1gE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "https://code.earthengine.google.com/82f224b504a3d90ac7038e010f6c7793"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkWk1Rz31lW6",
        "colab_type": "text"
      },
      "source": [
        "### Specify your Cloud Storage Bucket\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vSd57P-mUkAvVGyYjrsq-rQsSSIS4aTBN0mMvuX2o9PGxbWXjeEEU9GDSATr8aXZ0Fwp79a3Muypo4u/pub?w=100&amp;h=100\">\n",
        "\n",
        "You must have write access to a bucket to run this demo!  To run it read-only, use the demo bucket below, but note that writes to this bucket will not work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf5wFjXXb9wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is read-only:\n",
        "BUCKET = 'rice-mapping-unet'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYLGbmkI2NlF",
        "colab_type": "text"
      },
      "source": [
        "### Poygon Data Colab Global Variable Setup\n",
        "![alt text](https://colab.research.google.com/img/colab_favicon.ico) ![alt text](https://www.google.com/images/icons/product/earth_engine-128.png)\n",
        "\n",
        "No need to run these exports, but just to see how that's done:\n",
        "\n",
        "The block below used the pre-generated S2 composite saved as a GEE \n",
        "assest\n",
        "*  Resopnse is defined\n",
        "*  Features are defined\n",
        "-\n",
        "Key moving data across the Google Ecosystem is often a challenge \n",
        "\n",
        "---Tim move this languge above as key points talking about the changes associated with google ecosystem and patch data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdDKtN5o_6DV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify names locations for outputs in Cloud Storage. \n",
        "FOLDER = 'data'\n",
        "TRAINING_BASE = 'training_patches'\n",
        "EVAL_BASE = 'eval_patches'\n",
        "\n",
        "# Specify inputs (Landsat bands) to the model and the response variable.\n",
        "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "thermalBands = ['B10', 'B11']\n",
        "BANDS = opticalBands + thermalBands\n",
        "RESPONSE = 'rice'\n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "# Sizes of the training and evaluation datasets.\n",
        "TRAIN_SIZE = 16000\n",
        "EVAL_SIZE = 8000\n",
        "\n",
        "# Specify model training parameters.\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "BUFFER_SIZE = 2000\n",
        "OPTIMIZER = 'SGD'\n",
        "LOSS = 'MeanSquaredError'\n",
        "METRICS = ['RootMeanSquaredError']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8LFLHvqAJ6B",
        "colab_type": "text"
      },
      "source": [
        "### [GEE Script](https://code.earthengine.google.com/82f224b504a3d90ac7038e010f6c7793)  used to sample Polygons and generate TF record\n",
        "\n",
        "![alt text](https://www.google.com/images/icons/product/earth_engine-128.png) ![alt text](https://icons-for-free.com/iconfiles/png/128/key+password+unlock+icon-1320190846512238901.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2gpEpBY26IV",
        "colab_type": "text"
      },
      "source": [
        "### Imagery From GEE\n",
        " ![alt text](https://www.google.com/images/icons/product/earth_engine-128.png)![alt text](https://python-visualization.github.io/folium/_images/folium_logo.jpg)\n",
        "\n",
        "Gather and setup the imagery to use for inputs (predictors).  This is a three-year, cloud-free, Landsat 8 composite.  Display it in the notebook for a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU1CGl3Jb9oY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use Landsat 8 surface reflectance data.\n",
        "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
        "\n",
        "# Cloud masking function.\n",
        "def maskL8sr(image):\n",
        "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
        "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
        "  qa = image.select('pixel_qa')\n",
        "  mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
        "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
        "  mask2 = image.mask().reduce('min')\n",
        "  mask3 = image.select(opticalBands).gt(0).And(\n",
        "          image.select(opticalBands).lt(10000)).reduce('min')\n",
        "  mask = mask1.And(mask2).And(mask3)\n",
        "  return image.select(opticalBands).divide(10000).addBands(\n",
        "          image.select(thermalBands).divide(10).clamp(273.15, 373.15)\n",
        "            .subtract(273.15).divide(100)).updateMask(mask)\n",
        "\n",
        "# The image input data is a cloud-masked median composite.\n",
        "image = l8sr.filterDate('2018-01-01', '2018-12-31').map(maskL8sr).median()\n",
        "\n",
        "# Use folium to visualize the imagery.\n",
        "mapid = image.getMapId({'bands': ['B10'], 'min': 0, 'max': 0.5})\n",
        "folium.TileLayer(\n",
        "    tiles=EE_TILES.format(**mapid),\n",
        "    attr='Google Earth Engine',\n",
        "    overlay=True,\n",
        "    name='thermal',\n",
        "  ).add_to(map)\n",
        "\n",
        "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
        "map = folium.Map(location=[36.2037068,-113.7666616], zoom_start=6)\n",
        "folium.TileLayer(\n",
        "    tiles=EE_TILES.format(**mapid),\n",
        "    attr='Google Earth Engine',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcVHF_nD3gft",
        "colab_type": "text"
      },
      "source": [
        "### Prepare the Response\n",
        "![alt text](https://www.google.com/images/icons/product/earth_engine-128.png)![alt text](https://python-visualization.github.io/folium/_images/folium_logo.jpg)\n",
        "\n",
        "Prepare the response (what we want to predict).  This is USDA NASS Crop Layer,   display to check."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tX3PLrDb9W9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = ee.ImageCollection('USDA/NASS/CDL').filter(ee.Filter.date('2018-01-01', '2018-12-31')).first()\n",
        "cropLandcover = dataset.select('cropland')\n",
        "rice = cropLandcover.eq(3)\n",
        "rice = rice.updateMask(rice)\n",
        "rice = rice.unmask(0).rename([RESPONSE])\n",
        "\n",
        "map = folium.Map(location=[36.2037068,-113.7666616], zoom_start=6)\n",
        "mapid = rice.getMapId({'min': 0, 'max': 1, 'palette': 'yellow,red'})\n",
        "folium.TileLayer(\n",
        "    tiles=EE_TILES.format(**mapid),\n",
        "    attr='Google Earth Engine',\n",
        "    overlay=True,\n",
        "    name='rice',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wKkkgIn4FGQ",
        "colab_type": "text"
      },
      "source": [
        "### Create a single image\n",
        "\n",
        "![alt text](https://icons-for-free.com/iconfiles/png/128/key+password+unlock+icon-1320190846512238901.png)![alt text](https://www.google.com/images/icons/product/earth_engine-128.png)\n",
        "\n",
        "\n",
        "Stack the 2D images (Landsat composite and NASS crop layer) to create a single image from which samples can be taken.  Convert the image into an array image in which each pixel stores 256x256 patches of pixels for each band. \n",
        "\n",
        "**This is a key step that bears emphasis: to export training patches, convert a multi-band image to [an array image](https://developers.google.com/earth-engine/arrays_array_images#array-images) using [`neighborhoodToArray()`](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then sample the image at points.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzO2MKq84I6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "featureStack = ee.Image.cat([\n",
        "  image, rice\n",
        "]).float()\n",
        "\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
        "\n",
        "arrays = featureStack.neighborhoodToArray(kernel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq_AixIp5kVH",
        "colab_type": "text"
      },
      "source": [
        "## Training data\n",
        "\n",
        "![alt text](https://www.google.com/images/icons/product/earth_engine-128.png)![alt text](https://image.flaticon.com/icons/png/128/98/98673.png)<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">\n",
        "\n",
        "\n",
        "Load the data exported from Earth Engine into a `tf.data.Dataset`.  The following are helper functions for that.\n",
        "\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vTjGDbb6wQzLUKtFDgjkc9nXnF8X1dMXWK5OuYKVK8suuu1r1Hy3DCFWlyrgwhuU0MI7M5jfMcGXoco/pub?w=345&amp;h=332\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hvXZvs86sq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"The parsing function.\n",
        "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "  Args:\n",
        "    example_proto: a serialized Example.\n",
        "  Returns: \n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "  \"\"\"\n",
        "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple(inputs):\n",
        "  \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "  Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "  Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "  Returns: \n",
        "    A dtuple of (inputs, outputs).\n",
        "  \"\"\"\n",
        "  inputsList = [inputs.get(key) for key in FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  # Convert from CHW to HWC\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "\n",
        "def get_dataset(pattern):\n",
        "  \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
        "  Get all the files matching the pattern, parse and convert to tuple.\n",
        "  Args:\n",
        "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "  Returns: \n",
        "    A tf.data.Dataset\n",
        "  \"\"\"\n",
        "  glob = tf.gfile.Glob(pattern)\n",
        "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df9lKtMH76XO",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">\n",
        "\n",
        "Use the helpers to read in the training dataset.  Print the first record to check."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUwKszFf79bJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_training_dataset():\n",
        "\t\"\"\"Get the preprocessed training dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of training data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + BUCKET + '/' + FOLDER + '/' + TRAINING_BASE + '*'\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "training = get_training_dataset()\n",
        "\n",
        "print(iter(training.take(1)).next())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBdopuPQ9Srh",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation data\n",
        "\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">\n",
        "\n",
        "Now do the same thing to get an evaluation dataset.  Note that unlike the training dataset, **the evaluation dataset has a batch size of 1, is not repeated and is not shuffled.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_diGTqLg9ZNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_eval_dataset():\n",
        "\t\"\"\"Get the preprocessed evaluation dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of evaluation data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + BUCKET + '/' + FOLDER + '/' + EVAL_BASE + '*'\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.batch(1).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "evaluation = get_eval_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDWe9v4k95rD",
        "colab_type": "text"
      },
      "source": [
        "## U-Net Model\n",
        "\n",
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vTN4JZztednLUmsb9RFF9tayr9LjgQT_mZOcsdw0IoKTJtQyk-wfaSJurTj-TRZyww4unRacxDIaNih/pub?w=695&amp;h=263\">\n",
        "\n",
        "![alt text](https://colab.research.google.com/img/colab_favicon.ico)\n",
        "\n",
        "Here we use the Keras implementation of the U-Net model as found [in the TensorFlow examples](https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb).  The U-Net model takes 256x256 pixel patches as input and outputs per-pixel class probability, label or a continuous output.  We can implement the model essentially unmodified, but will use mean squared error loss on the sigmoidal output since we are treating this as a regression problem, rather than a classification problem.  Since impervious surface fraction is constrained to [0,1], with many values close to zero or one, a saturating activation function is suitable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrgDnZZA99pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import metrics\n",
        "from tensorflow.python.keras import optimizers\n",
        "\n",
        "def conv_block(input_tensor, num_filters):\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\treturn encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "\tencoder = conv_block(input_tensor, num_filters)\n",
        "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "\treturn encoder_pool, encoder\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\treturn decoder\n",
        "\n",
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[None, None, len(BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
        "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
        "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
        "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
        "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
        "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
        "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
        "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
        "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
        "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
        "\toutputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)\n",
        "\n",
        "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=optimizers.get(OPTIMIZER), \n",
        "\t\tloss=losses.get(LOSS),\n",
        "\t\tmetrics=[metrics.get(metric) for metric in METRICS])\n",
        "\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2CVHEw-Cig9",
        "colab_type": "text"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "\n",
        "![alt text](https://colab.research.google.com/img/colab_favicon.ico)\n",
        "\n",
        "You train a Keras model by calling `.fit()` on it.  Here we're going to train for 10 epochs, which is suitable for demonstration purposes.  For production use, you probably want to optimize this parameter, for example through [hyperparamter tuning](https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCKNV5YHDDhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = get_model()\n",
        "\n",
        "training = m.fit(\n",
        "    x=training, \n",
        "    epochs=EPOCHS, \n",
        "    steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE), \n",
        "    validation_data=evaluation,\n",
        "    validation_steps=EVAL_SIZE\n",
        ")\n",
        "\n",
        "%pylab inline\n",
        "plot(training.history['loss'], 'x--')\n",
        "plot(training.history['acc'], 'o--')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwUFIWCzB_mi",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">\n",
        "\n",
        "Note that the notebook VM is sometimes not heavy-duty enough to get through a whole training job, especially if you have a large buffer size or a large number of epochs.  You can still use this notebook for training, but may need to set up an alternative VM ([learn more](https://research.google.com/colaboratory/local-runtimes.html)) for production use.  Alternatively, you can package your code for running large training jobs on Google's AI Platform [as described here](https://cloud.google.com/ml-engine/docs/tensorflow/trainer-considerations).  The following code loads a pre-trained model, which you can use for predictions right away."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpb3nF8fDh6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save a trained model\n",
        "MODEL_DIR = 'gs://' + BUCKET + '/model/' \n",
        "tf.contrib.saved_model.save_keras_model(model, MODEL_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbZf2OiiDil6",
        "colab_type": "text"
      },
      "source": [
        "## Prediction\n",
        "\n",
        "![alt text](https://www.google.com/images/icons/product/earth_engine-128.png)![alt text](https://image.flaticon.com/icons/png/128/98/98673.png)<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQp5prD8q1GrT-1x5bKBnYx8hZBWGhB7SDNJhAaJVPxdziFSSp1fi39UR58mC-Ii62789hPHKgmNEST/pub?w=100&amp;h=100\">\n",
        "\n",
        "\n",
        "The prediction pipeline is:\n",
        "\n",
        "1.  Export imagery on which to do predictions from Earth Engine in TFRecord format to a Cloud Storge bucket.\n",
        "2.  Use the trained model to make the predictions.\n",
        "3.  Write the predictions to a TFRecord file in a Cloud Storage.\n",
        "4.  Upload the predictions TFRecord file to Earth Engine.\n",
        "\n",
        "The following functions handle this process.  It's useful to separate the export from the predictions so that you can experiment with different models without running the export every time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnL794feD7uv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doExport(out_image_base, kernel_buffer, region):\n",
        "  \"\"\"Run the image export task.  Block until complete.\n",
        "  \"\"\"\n",
        "  task = ee.batch.Export.image.toCloudStorage(\n",
        "    image = image.select(BANDS), \n",
        "    description = out_image_base, \n",
        "    bucket = BUCKET, \n",
        "    fileNamePrefix = FOLDER + '/' + out_image_base, \n",
        "    region = region.getInfo()['coordinates'], \n",
        "    scale = 30, \n",
        "    fileFormat = 'TFRecord', \n",
        "    maxPixels = 1e10,\n",
        "    formatOptions = { \n",
        "      'patchDimensions': KERNEL_SHAPE,\n",
        "      'kernelSize': kernel_buffer,\n",
        "      'compressed': True,\n",
        "      'maxFileSize': 104857600\n",
        "    }\n",
        "  )\n",
        "  task.start()\n",
        "\n",
        "  # Block until the task completes.\n",
        "  print('Running image export to Cloud Storage...')\n",
        "  import time\n",
        "  while task.active():\n",
        "    time.sleep(30)\n",
        "\n",
        "  # Error condition\n",
        "  if task.status()['state'] != 'COMPLETED':\n",
        "    print('Error with image export.')\n",
        "  else:\n",
        "    print('Image export completed.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp8eyx-SEHTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doPrediction(out_image_base, user_folder, kernel_buffer, region):\n",
        "  \"\"\"Perform inference on exported imagery, upload to Earth Engine.\n",
        "  \"\"\"\n",
        "\n",
        "  print('Looking for TFRecord files...')\n",
        "  \n",
        "  # Get a list of all the files in the output bucket.\n",
        "  filesList = !gsutil ls 'gs://'{BUCKET}'/'{FOLDER}\n",
        "  # Get only the files generated by the image export.\n",
        "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
        "\n",
        "  # Get the list of image files and the JSON mixer file.\n",
        "  imageFilesList = []\n",
        "  jsonFile = None\n",
        "  for f in exportFilesList:\n",
        "    if f.endswith('.tfrecord.gz'):\n",
        "      imageFilesList.append(f)\n",
        "    elif f.endswith('.json'):\n",
        "      jsonFile = f\n",
        "\n",
        "  # Make sure the files are in the right order.\n",
        "  imageFilesList.sort()\n",
        "\n",
        "  from pprint import pprint\n",
        "  pprint(imageFilesList)\n",
        "  print(jsonFile)\n",
        "  \n",
        "  import json\n",
        "  # Load the contents of the mixer file to a JSON object.\n",
        "  jsonText = !gsutil cat {jsonFile}\n",
        "  # Get a single string w/ newlines from the IPython.utils.text.SList\n",
        "  mixer = json.loads(jsonText.nlstr)\n",
        "  pprint(mixer)\n",
        "  patches = mixer['totalPatches']\n",
        "  \n",
        "  # Get set up for prediction.\n",
        "  x_buffer = int(kernel_buffer[0] / 2)\n",
        "  y_buffer = int(kernel_buffer[1] / 2)\n",
        "\n",
        "  buffered_shape = [\n",
        "      KERNEL_SHAPE[0] + kernel_buffer[0],\n",
        "      KERNEL_SHAPE[1] + kernel_buffer[1]]\n",
        "\n",
        "  imageColumns = [\n",
        "    tf.FixedLenFeature(shape=buffered_shape, dtype=tf.float32) \n",
        "      for k in BANDS\n",
        "  ]\n",
        "\n",
        "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
        "\n",
        "  def parse_image(example_proto):\n",
        "    return tf.parse_single_example(example_proto, imageFeaturesDict)\n",
        "\n",
        "  def toTupleImage(dict):\n",
        "    inputsList = [dict.get(key) for key in BANDS]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n",
        "  \n",
        "   # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
        "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
        "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
        "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
        "  \n",
        "  # Perform inference.\n",
        "  print('Running predictions...')\n",
        "  predictions = m.predict(imageDataset, steps=patches, verbose=1)\n",
        "  # print(predictions[0])\n",
        "\n",
        "  print('Writing predictions...')\n",
        "  out_image_file = 'gs://' + BUCKET + '/' + FOLDER + '/' + out_image_base + '.TFRecord'\n",
        "  writer = tf.python_io.TFRecordWriter(out_image_file)\n",
        "  patches = 0\n",
        "  for predictionPatch in predictions:\n",
        "    print('Writing patch ' + str(patches) + '...')\n",
        "    predictionPatch = predictionPatch[\n",
        "        x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
        "\n",
        "    # Create an example.\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'impervious': tf.train.Feature(\n",
        "              float_list=tf.train.FloatList(\n",
        "                  value=predictionPatch.flatten()))\n",
        "        }\n",
        "      )\n",
        "    )\n",
        "    # Write the example.\n",
        "    writer.write(example.SerializeToString())\n",
        "    patches += 1\n",
        "\n",
        "  writer.close()\n",
        "\n",
        "  # Start the upload.\n",
        "  out_image_asset = user_folder + '/' + out_image_base\n",
        "  !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IgOa_HTFEZy",
        "colab_type": "text"
      },
      "source": [
        "### Final Step for the Prediction\n",
        "Now there's all the code needed to run the prediction pipeline, all that remains is to:\n",
        "*  specify the output region in which to do the prediction,\n",
        "*  the names of the output files,\n",
        "*  where to put them,\n",
        "*  and the shape of the outputs.  \n",
        "\n",
        "In terms of the shape, the model is trained on 256x256 patches, but can work (in theory) on any patch that's big enough with even dimensions ([reference](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)).  \n",
        "\n",
        "![alt text](https://icons-for-free.com/iconfiles/png/128/key+password+unlock+icon-1320190846512238901.png)\n",
        "\n",
        "Because of tile boundary artifacts, give the model slightly larger patches for prediction, then clip out the middle 256x256 patch.  This is controlled with a kernel buffer, half the size of which will extend beyond the kernel buffer.  For example, specifying a 128x128 kernel will append 64 pixels on each side of the patch, to ensure that the pixels in the output are taken from inputs completely covered by the kernel.  \n",
        "\n",
        "Specifc your file paths ways as needed...see the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IntjGPjWF0U3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This has a read-only asset in it:\n",
        "user_folder = 'users/Enteryourfilenamehere'\n",
        "\n",
        "# Base file name to use for TFRecord files and assets.\n",
        "rice_image_base = 'tf_demo2_rice_'\n",
        "# Half this will extend on the sides of each patch.\n",
        "rice_kernel_buffer = [128, 128]\n",
        "# Beijing\n",
        "rice_region = ee.Geometry.Polygon(\n",
        "        [[[115.9662455210937, 40.121362012835235],\n",
        "          [115.9662455210937, 39.64293313749715],\n",
        "          [117.01818643906245, 39.64293313749715],\n",
        "          [117.01818643906245, 40.121362012835235]]], None, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TFN3M2aF2fj",
        "colab_type": "text"
      },
      "source": [
        "## Display the output\n",
        "\n",
        "One the data has been exported, the model has made predictions and the predictions have been written to a file, and the image imported to Earth Engine, it's possible to display the resultant Earth Engine asset.  Here, display the impervious area predictions over Beijing, China."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q4XgZBKGMEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This has a read-only asset in it:\n",
        "user_folder = 'users/Enteryourfilenamehere'\n",
        "\n",
        "# Base file name to use for TFRecord files and assets.\n",
        "rice_image_base = 'tf_demo2_rice_'\n",
        "# Half this will extend on the sides of each patch.\n",
        "rice_kernel_buffer = [128, 128]\n",
        "# Beijing\n",
        "rice_region = ee.Geometry.Polygon(\n",
        "        [[[115.9662455210937, 40.121362012835235],\n",
        "          [115.9662455210937, 39.64293313749715],\n",
        "          [117.01818643906245, 39.64293313749715],\n",
        "          [117.01818643906245, 40.121362012835235]]], None, False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}